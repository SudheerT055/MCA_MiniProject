{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "00158e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.of docx files detected:  6\n",
      "['1.docx', '2 - Copy.docx', '2.docx', '3.docx', '4.docx', '5.docx']\n",
      "['\\n1.Unit Level Testing:-\\n--------------------\\nA smallest testing part(or)portion in the same code of the application is called as \"Unit testing\".\\n--->Each and every part of unit testing working(or)not\\n-->Unit Testing is also known as \"Module Testing\".', '1.Top-Down approach:-\\n-----------------------\\nThis approach is recommened if there are any incomplete program at bottom level,in this approach integration testing will be carried out from top to bottom,the incomplete will be replaced with \"STUB\".', '1.Top-Down approach:-\\n-----------------------\\nThis approach is recommened if there are any incomplete program at bottom level,in this approach integration testing will be carried out from top to bottom,the incomplete will be replaced with \"STUB\".', '3.Hybrid Approach :-\\n------------------\\nIt is a combination of Top-down and bottom-up approach.\\n-->The incomplete program will be replaced botH STUB an Driver.', 'Blackbox Testing:-\\n-----------------\\nOnce 100% is coding is completed and white box completed to confirm correctness of requirement on the application is called as \"Balck Box testing\".\\n\\n--->This testing is conducted by testing team/testers.\\n\\n--->In Blackbox it is divided into 2 Levels.\\n1.System Level Testing\\n2.UAT (User Acceptance testing)', 'Coming to Functional testing it is divided into multiple types of testing\\n\\n1.Smoke testing\\n2.Positive Testing\\n3.Negative testing\\n4.Re-testing\\n5.Regression testing\\n6.End-To -End testing\\n7.Adhoc Testing']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "import docx\n",
    "from docx import Document\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "\n",
    "#doc file input in python\n",
    "def getText(docfilename):\n",
    "    doc = Document(docfilename)\n",
    "    fulltext=[]\n",
    "    for para in doc.paragraphs:\n",
    "        fulltext.append(para.text)\n",
    "#     print(\"\\n \".join(fulltext))\n",
    "    return \"\\n\".join(fulltext)\n",
    "#input filepath where all assignment belong\n",
    "docFiles=[]\n",
    "docFilesList =[]\n",
    "for docfilename in os.listdir(r\"C:\\Users\\Sudheer\\GIT\\BMSP\"):\n",
    "    if (not docfilename.startswith('~')) & docfilename.endswith('.docx') :\n",
    "        docFilesList.append(docfilename)\n",
    "        docFiles.append(getText(docfilename))\n",
    "docFiles.sort(key=str.lower)\n",
    "print(\"No.of docx files detected: \",len(docFiles))\n",
    "print(docFilesList)\n",
    "print(docFiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "fa8e1975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building vocabulary of all the docuents\n",
    "def build_lexicon(corpus):\n",
    "    lexicon=set()\n",
    "    for doc in corpus:\n",
    "        #word tokenization\n",
    "        word_token=[word for word in doc.split()]\n",
    "        lower_word_list=[i.lower() for i in word_token]\n",
    "        #stemming\n",
    "        porter = PorterStemmer()\n",
    "        stemmed_word=[porter.stem(t) for t in lower_word_list]\n",
    "        #removing stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_bag_of_word=[w for w in stemmed_word if not w in stop_words]\n",
    "        print(filtered_bag_of_word)\n",
    "        lexicon.update(filtered_bag_of_word)\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "294db315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.unit', 'level', 'testing:-', '--------------------', 'smallest', 'test', 'part(or)port', 'code', 'applic', 'call', '\"unit', 'testing\".', '--->each', 'everi', 'part', 'unit', 'test', 'working(or)not', '-->unit', 'test', 'also', 'known', '\"modul', 'testing\".']\n",
      "['1.top-down', 'approach:-', '-----------------------', 'thi', 'approach', 'recommen', 'ani', 'incomplet', 'program', 'bottom', 'level,in', 'thi', 'approach', 'integr', 'test', 'carri', 'top', 'bottom,th', 'incomplet', 'replac', '\"stub\".']\n",
      "['1.top-down', 'approach:-', '-----------------------', 'thi', 'approach', 'recommen', 'ani', 'incomplet', 'program', 'bottom', 'level,in', 'thi', 'approach', 'integr', 'test', 'carri', 'top', 'bottom,th', 'incomplet', 'replac', '\"stub\".']\n",
      "['3.hybrid', 'approach', ':-', '------------------', 'combin', 'top-down', 'bottom-up', 'approach.', '-->the', 'incomplet', 'program', 'replac', 'stub', 'driver.']\n",
      "['blackbox', 'testing:-', '-----------------', 'onc', '100%', 'code', 'complet', 'white', 'box', 'complet', 'confirm', 'correct', 'requir', 'applic', 'call', '\"balck', 'box', 'testing\".', '--->thi', 'test', 'conduct', 'test', 'team/testers.', '--->in', 'blackbox', 'divid', '2', 'levels.', '1.system', 'level', 'test', '2.uat', '(user', 'accept', 'testing)']\n",
      "['come', 'function', 'test', 'divid', 'multipl', 'type', 'test', '1.smoke', 'test', '2.posit', 'test', '3.neg', 'test', '4.re-test', '5.regress', 'test', '6.end-to', '-end', 'test', '7.adhoc', 'test']\n"
     ]
    }
   ],
   "source": [
    "vocabulary=build_lexicon(docFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "755da265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(term, document):\n",
    "    return freq(term, document)\n",
    "def freq(term, document):\n",
    "    return document.split().count(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "bfd0873d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Our Vocabulary vector is [part,bottom,th,-end,working(or)not,testing\".,-->the,1.smoke,blackbox,box,smallest,team/testers.,program,\"unit,5.regress,-->unit,onc,complet,-----------------------,top-down,stub,combin,\"modul,2.posit,------------------,\"stub\".,2,known,ani,--->in,white,thi,--------------------,approach:-,recommen,part(or)port,-----------------,2.uat,call,1.unit,bottom-up,--->each,also,divid,level,in,approach,replac,accept,confirm,testing:-,bottom,correct,integr,3.hybrid,3.neg,come,1.system,conduct,levels.,(user,unit,requir,type,100%,\"balck,1.top-down,multipl,testing),7.adhoc,function,6.end-to,4.re-test,applic,top,:-,level,approach.,everi,driver.,code,test,--->thi,carri,incomplet] \n",
      "\n",
      " the tf vector for document 1 is [1,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0]\n",
      "\n",
      " the tf vector for document 2 is [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,2,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0]\n",
      "\n",
      " the tf vector for document 2 is [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,2,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0]\n",
      "\n",
      " the tf vector for document 4 is [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0]\n",
      "\n",
      " the tf vector for document 5 is [0,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
      "\n",
      " the tf vector for document 6 is [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
      "\n",
      " All combined here is our master document term matrix:\n",
      "[[1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "doc_term_matrix=[]\n",
    "print(\"\\n Our Vocabulary vector is [\"+','.join(list(vocabulary)) + \"] \")\n",
    "for doc in docFiles:\n",
    "    tf_vector=[tf(word, doc) for word in vocabulary]\n",
    "    tf_vector_string=\",\".join(format (freq, 'd') for freq in tf_vector)\n",
    "    print(\"\\n the tf vector for document %d is [%s]\" % ((docFiles.index(doc)+1),tf_vector_string))\n",
    "    doc_term_matrix.append(tf_vector)\n",
    "print(\"\\n All combined here is our master document term matrix:\")\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452f8f80",
   "metadata": {},
   "source": [
    "#Now every document is in the same feature space\n",
    "#Normalizing vectors to  norm\n",
    "#l2 norm of each vector is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "9cc8021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalizer(vec):\n",
    "    denom = np.sum([el**2 for el in vec])\n",
    "    if math.sqrt(denom):\n",
    "        return [(el/math.sqrt(denom)) for el in vec]\n",
    "    else:\n",
    "        return[el for el in vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3edbe396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " A regular old document term matrix: \n",
      "[[1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 1 2 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 1 2 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      "A document term matrix with row wise l2 norms of 1: \n",
      "[[0.33333333 0.         0.         0.33333333 0.33333333 0.\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31622777\n",
      "  0.         0.         0.         0.         0.         0.31622777\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.31622777 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.31622777 0.63245553 0.         0.         0.\n",
      "  0.         0.31622777 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.31622777 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31622777\n",
      "  0.         0.         0.         0.         0.         0.31622777\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.31622777 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.31622777 0.63245553 0.         0.         0.\n",
      "  0.         0.31622777 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.31622777 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.4472136\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.4472136\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.4472136  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.4472136  0.         0.4472136  0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.         0.33333333 0.         0.33333333 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "doc_term_matrix_l2=[]\n",
    "for vec in doc_term_matrix:\n",
    "    doc_term_matrix_l2.append(l2_normalizer (vec))\n",
    "print(\"\\n A regular old document term matrix: \")\n",
    "print(np.matrix(doc_term_matrix))\n",
    "print(\"\\nA document term matrix with row wise l2 norms of 1: \")\n",
    "print(np.matrix(doc_term_matrix_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "539189ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numDocsContaining (word, doclist):\n",
    "    doccount =0\n",
    "    for doc in doclist:\n",
    "        if freq(word, doc)>0:\n",
    "            doccount +1\n",
    "    return doccount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6ddab54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf (word,doclist):\n",
    "    n_samples=len(doclist)\n",
    "    df= numDocsContaining (word, doclist)\n",
    "    return np.log(n_samples/1+df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "68c5b39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our vocabuary vector is[ part,bottom,th,-end,working(or)not,testing\".,-->the,1.smoke,blackbox,box,smallest,team/testers.,program,\"unit,5.regress,-->unit,onc,complet,-----------------------,top-down,stub,combin,\"modul,2.posit,------------------,\"stub\".,2,known,ani,--->in,white,thi,--------------------,approach:-,recommen,part(or)port,-----------------,2.uat,call,1.unit,bottom-up,--->each,also,divid,level,in,approach,replac,accept,confirm,testing:-,bottom,correct,integr,3.hybrid,3.neg,come,1.system,conduct,levels.,(user,unit,requir,type,100%,\"balck,1.top-down,multipl,testing),7.adhoc,function,6.end-to,4.re-test,applic,top,:-,level,approach.,everi,driver.,code,test,--->thi,carri,incomplet]\n"
     ]
    }
   ],
   "source": [
    "my_idf_vector=[idf(word, docFiles) for word in vocabulary]\n",
    "print(\"our vocabuary vector is[ \"+ \",\".join(list (vocabulary)) + \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "2d0dc4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_idf_matrix(idf_vector):\n",
    "    idf_mat=np.zeros((len(idf_vector), len(idf_vector)))\n",
    "    np.fill_diagonal(idf_mat,idf_vector)\n",
    "    return idf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "bafa8872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Idf matrix is:\n",
      " [[1.79175947 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         1.79175947 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         1.79175947 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 1.79175947 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         1.79175947 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         1.79175947]]\n"
     ]
    }
   ],
   "source": [
    "my_idf_matrix=build_idf_matrix(my_idf_vector)\n",
    "print(\"\\nIdf matrix is:\\n\",my_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "6ec456b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix_tfidf=[]\n",
    "#performing tfidf matrix multiplication\n",
    "for tf_vector in doc_term_matrix:\n",
    "    doc_term_matrix_tfidf.append(np.dot(tf_vector,my_idf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "432ff058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'part', 'bottom,th', '-end', 'working(or)not', 'testing\".', '-->the', '1.smoke', 'blackbox', 'box', 'smallest', 'team/testers.', 'program', '\"unit', '5.regress', '-->unit', 'onc', 'complet', '-----------------------', 'top-down', 'stub', 'combin', '\"modul', '2.posit', '------------------', '\"stub\".', '2', 'known', 'ani', '--->in', 'white', 'thi', '--------------------', 'approach:-', 'recommen', 'part(or)port', '-----------------', '2.uat', 'call', '1.unit', 'bottom-up', '--->each', 'also', 'divid', 'level,in', 'approach', 'replac', 'accept', 'confirm', 'testing:-', 'bottom', 'correct', 'integr', '3.hybrid', '3.neg', 'come', '1.system', 'conduct', 'levels.', '(user', 'unit', 'requir', 'type', '100%', '\"balck', '1.top-down', 'multipl', 'testing)', '7.adhoc', 'function', '6.end-to', '4.re-test', 'applic', 'top', ':-', 'level', 'approach.', 'everi', 'driver.', 'code', 'test', '--->thi', 'carri', 'incomplet'}\n",
      "[[0.33333333 0.         0.         0.33333333 0.33333333 0.\n",
      "  0.         0.         0.         0.33333333 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.33333333 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31622777\n",
      "  0.         0.         0.         0.         0.         0.31622777\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.31622777 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.31622777 0.63245553 0.         0.         0.\n",
      "  0.         0.31622777 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.31622777 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31622777\n",
      "  0.         0.         0.         0.         0.         0.31622777\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.31622777 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.31622777 0.63245553 0.         0.         0.\n",
      "  0.         0.31622777 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.31622777 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.4472136\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.4472136\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.4472136  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.4472136  0.         0.4472136  0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.33333333 0.\n",
      "  0.         0.         0.33333333 0.         0.33333333 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.33333333 0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33333333\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.         0.         0.\n",
      "  0.33333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "doc_term_matrix_tfidf_l2=[]\n",
    "for tf_vector in doc_term_matrix_tfidf:\n",
    "    doc_term_matrix_tfidf_l2.append(l2_normalizer(tf_vector))\n",
    "print(vocabulary)\n",
    "print(np.matrix(doc_term_matrix_tfidf_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e81f03f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.docx', '2 - Copy.docx', '2.docx', '3.docx', '4.docx', '5.docx']\n",
      "\n",
      " Cosine Distance btw doc 0 and doc 1:\n",
      "1.0\n",
      "\n",
      "Plagiarism = 0%\n",
      "\n",
      " Cosine Distance btw doc 0 and doc 2:\n",
      "1.0\n",
      "\n",
      "Plagiarism = 0%\n",
      "\n",
      " Cosine Distance btw doc 0 and doc 3:\n",
      "1.0\n",
      "\n",
      "Plagiarism = 0%\n",
      "\n",
      " Cosine Distance btw doc 0 and doc 4:\n",
      "0.8888888888888888\n",
      "\n",
      "Plagiarism = 11%\n",
      "\n",
      " Cosine Distance btw doc 0 and doc 5:\n",
      "nan\n",
      "\n",
      " Plagiarism = 0%\n",
      "\n",
      " Cosine Distance btw doc 1 and doc 2:\n",
      "0.0\n",
      "\n",
      "Plagiarism = 100%\n",
      "\n",
      " Cosine Distance btw doc 1 and doc 3:\n",
      "0.8585786437626904\n",
      "\n",
      "Plagiarism = 14%\n",
      "\n",
      " Cosine Distance btw doc 1 and doc 4:\n",
      "1.0\n",
      "\n",
      "Plagiarism = 0%\n",
      "\n",
      " Cosine Distance btw doc 1 and doc 5:\n",
      "nan\n",
      "\n",
      " Plagiarism = 0%\n",
      "\n",
      " Cosine Distance btw doc 2 and doc 3:\n",
      "0.8585786437626904\n",
      "\n",
      "Plagiarism = 14%\n",
      "\n",
      " Cosine Distance btw doc 2 and doc 4:\n",
      "1.0\n",
      "\n",
      "Plagiarism = 0%\n",
      "\n",
      " Cosine Distance btw doc 2 and doc 5:\n",
      "nan\n",
      "\n",
      " Plagiarism = 0%\n",
      "\n",
      " Cosine Distance btw doc 3 and doc 4:\n",
      "1.0\n",
      "\n",
      "Plagiarism = 0%\n",
      "\n",
      " Cosine Distance btw doc 3 and doc 5:\n",
      "nan\n",
      "\n",
      " Plagiarism = 0%\n",
      "\n",
      " Cosine Distance btw doc 4 and doc 5:\n",
      "nan\n",
      "\n",
      " Plagiarism = 0%\n"
     ]
    }
   ],
   "source": [
    "print(docFilesList)\n",
    "for i in range(len(docFiles)):\n",
    "    for j in range(i+1, len(docFiles)):\n",
    "        result_nltk = nltk.cluster.util.cosine_distance(doc_term_matrix_tfidf_l2[i],doc_term_matrix_tfidf_l2[j])\n",
    "        print(\"\\n Cosine Distance btw doc %d and doc %d:\" %(i,j))\n",
    "        print (result_nltk)\n",
    "        cos_sin = 1 - result_nltk\n",
    "        try:\n",
    "            angle_in_radians=math.acos(cos_sin)\n",
    "        except ValueError:\n",
    "            print(\"Here Error\")\n",
    "        if not math.isnan(cos_sin):\n",
    "            plagiarism=int(cos_sin*100)\n",
    "            print(\"\\nPlagiarism = \" + str(plagiarism) +\"%\")\n",
    "        else:\n",
    "            print(\"\\n Plagiarism = 0%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a88460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
